{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import pyphen # used to split word into phonetic syllable. We can treat \n",
    "# the words as Italian which has strict phonetic structure \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: We create wordbags from scraped web data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split texts of chosen language into syllables using Pyphen library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PhoneticWordSplit(word):\n",
    "    dic = pyphen.Pyphen(lang='it_IT')\n",
    "    splitword = dic.inserted(word)\n",
    "    splitword = splitword.replace(\"-\", \" \")\n",
    "    return splitword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WordBagMaker(corpusfilename, label):\n",
    "    f = open(corpusfilename, 'r')\n",
    "    lines = f.readlines()\n",
    "\n",
    "    import random\n",
    "    worddict = []\n",
    "    for l in lines:\n",
    "        words = l.split()\n",
    "        #only words present in sentences used, not single word sentences\n",
    "        if (len(words) < 2): continue \n",
    "        for w in words:\n",
    "            #words with digits not used and single letter words not used\n",
    "            if (len(w) > 1) and w.isalpha(): \n",
    "                worddict.append(PhoneticWordSplit(w) + \" : \" + label)\n",
    "    #shuffle the list\n",
    "    random.shuffle(worddict)   \n",
    "    return worddict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a helper function that writes wordbag of a given language \n",
    "def writeWordbag(corpulsFilename, wordbagFilename, label):\n",
    "    fl = open(wordbagFilename, \"x\") #create a new file and write to it\n",
    "    r = WordBagMaker(corpusfilename= corpulsFilename, label= label) # returns an encoding eg \"ami eka : B\"\n",
    "    for _ in r:\n",
    "        fl.write(_+\"\\n\")\n",
    "    fl.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = [\"Korean\", \"Bangla\"]\n",
    "labelMap = {\"Korean\": \"K\", \"Bangla\": \"B\"}\n",
    "for thisLang in  lang: \n",
    "    #read from corpus and create word bag \n",
    "    #! change filename from sample\n",
    "    corpusFilename = f'{thisLang}Corpus.txt'\n",
    "    \n",
    "    wordbagFilename = f'{thisLang}Wordbag.txt'\n",
    "    writeWordbag(corpusFilename, wordbagFilename, labelMap[thisLang])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "databan = open(\"BanglaWordbag.txt\", 'r').readlines()\n",
    "datakor = open(\"KoreanWordbag.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bangla words mostly have 2 syllables, Korean words can have upto 5 syllables in a word. A 10 syllable vector should be enough to encode a word from either language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 50000\n",
    "Ntest  = 10000\n",
    "random.shuffle(datakor) #korean\n",
    "random.shuffle(databan) #bangla\n",
    "TrainingVal_data = datakor[:Ntrain] + databan[:Ntrain] \n",
    "Testing_data = datakor[Ntrain:Ntrain+Ntest] + databan[Ntrain:Ntrain+Ntest]\n",
    "random.shuffle(TrainingVal_data)\n",
    "random.shuffle(Testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "288bf4cd8390b4d9c87323c2ab08eb529ab0ea5f3c2699f7e898de53780fa45e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
